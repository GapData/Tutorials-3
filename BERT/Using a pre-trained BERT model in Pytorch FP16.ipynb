{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An excellent Pytorch implementation of the BERT model was done by [HuggingFace](https://github.com/huggingface/pytorch-transformers). They also included supplementary code to build upon pre-trained BERT models for a wide range of NLP tasks. Below is a detailed walkthrough of their code that is needed to fine-tune a trained BERT model for a Question Answering task (to be run on  [the SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/)).\n",
    "\n",
    "Except for the *Demo* section of this notebook, and a few minor edits and comments added here and there, the code was pasted directly from various files in HuggingFace's [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) GitHub repository. I focused on the part of the code that is task-specific (in this case, that means question answering). Below I assume that you have already performed\n",
    "\n",
    "```\n",
    "! pip install pytorch-transformers\n",
    "``` \n",
    "\n",
    "and copied the SQuAD 1.0 [train](https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json) and [dev](https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json) datasets, as well as the [evaluation script](https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py), to a directory called `/root/BERT/SQuAD1`. I also copied the BERT vocab file from [here](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt) to the same directory as the Notebook.\n",
    "\n",
    "This is an FP16 (half-precision) version of the model which was trained in **mixed precision** using [NVIDIA's apex](https://github.com/NVIDIA/apex). [More on mixed precision training here.](https://www.linkedin.com/pulse/joys-mixed-precision-training-ml-olga-petrova/)\n",
    "\n",
    "\n",
    "# 1. Data preprocessing\n",
    "\n",
    "The SQuAD dataset contains contexts (passages of text containing multiple sentences), questions (sentences), and answers (uninterrupted sections of a single sentence from a context answering a particular question). These are organized via multiple layers of ID numbers (grouped by multiple-contexts-with-a-common-theme / context-with-multiple-corresponding-QAs / question with acceptable answers: single possible answer in `train-v1.1.json` and generally multiple correct answers in `dev-v1.1.json`).\n",
    "\n",
    "### 1A. Load the SQuAD data:\n",
    "\n",
    "First, we need to be able to load the training/test examples from the `json` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load SQuAD dataset. \"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import collections\n",
    "from io import open\n",
    "\n",
    "from pytorch_transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize\n",
    "\n",
    "class SquadExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for the Squad dataset.\n",
    "    For examples without an answer, the start and end position are -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 question_text,\n",
    "                 doc_tokens,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"qas_id: %s\" % (self.qas_id)\n",
    "        s += \", question_text: %s\" % (\n",
    "            self.question_text)\n",
    "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "        if self.orig_answer_text:\n",
    "            s += \", orig_answer_text: %s\" % (self.orig_answer_text)\n",
    "        if self.start_position:\n",
    "            s += \", start_position: %d\" % (self.start_position)\n",
    "        if self.end_position:\n",
    "            s += \", end_position: %d\" % (self.end_position)\n",
    "        if self.is_impossible:\n",
    "            s += \", is_impossible: %r\" % (self.is_impossible)\n",
    "        return s\n",
    "    \n",
    "def read_squad_examples(input_file, is_training, version_2_with_negative):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph_text:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "                is_impossible = False\n",
    "                if is_training:\n",
    "                    if version_2_with_negative:\n",
    "                        is_impossible = qa[\"is_impossible\"]\n",
    "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\")\n",
    "                    if not is_impossible:\n",
    "                        answer = qa[\"answers\"][0]\n",
    "                        orig_answer_text = answer[\"text\"]\n",
    "                        answer_offset = answer[\"answer_start\"]\n",
    "                        answer_length = len(orig_answer_text)\n",
    "                        start_position = char_to_word_offset[answer_offset]\n",
    "                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
    "                        # Only add answers where the text can be exactly recovered from the\n",
    "                        # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                        # stuff so we will just skip the example.\n",
    "                        #\n",
    "                        # Note that this means for training mode, every example is NOT\n",
    "                        # guaranteed to be preserved.\n",
    "                        actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
    "                        cleaned_answer_text = \" \".join(\n",
    "                            whitespace_tokenize(orig_answer_text))\n",
    "                        if actual_text.find(cleaned_answer_text) == -1:\n",
    "                            logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                           actual_text, cleaned_answer_text)\n",
    "                            continue\n",
    "                    else:\n",
    "                        start_position = -1\n",
    "                        end_position = -1\n",
    "                        orig_answer_text = \"\"\n",
    "\n",
    "                example = SquadExample(\n",
    "                    qas_id=qas_id,\n",
    "                    question_text=question_text,\n",
    "                    doc_tokens=doc_tokens,\n",
    "                    orig_answer_text=orig_answer_text,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=is_impossible)\n",
    "                examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qas_id: 5733be284776f41900661180, question_text: The Basilica of the Sacred heart at Notre Dame is beside to which structure?, doc_tokens: [Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.], orig_answer_text: the Main Building, start_position: 49, end_position: 51"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = \"/root/BERT/SQuAD1/train-v1.1.json\"\n",
    "train_examples = read_squad_examples(train_file, True, False)\n",
    "# Note that the dev_file can only be read via read_squad_examples(train_file, False, False)\n",
    "# due to there being more than one possible answer\n",
    "train_examples[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. Tokenize the text:\n",
    "\n",
    "The contexts, questions, and answers need to be **tokenized** (projected onto the basis used by BERT to represent words). BERT makes use of [wordpiece tokenization](https://arxiv.org/pdf/1609.08144.pdf), which minimizes the number of *unknown* words by breaking them further into subparts. They also use a longest-match-first algorithm to perform tokenization, such that words contained in the vocabularly will not be broken down, whereas e.g. *unaffable*, which is not part of the vocabularly, will return the following three word pieces that are in it: \\[*un*, *##aff*, *##able*\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', '##ization', 'is', 'the', 'act', 'of', 'breaking', 'up', 'a', 'sequence', 'of', 'strings', 'into', 'pieces', 'such', 'as', 'words', ',', 'key', '##words', ',', 'phrases', ',', 'symbols', 'and', 'other', 'elements', 'called', 'token', '##s', '.']\n",
      "[19204, 3989, 2003, 1996, 2552, 1997, 4911, 2039, 1037, 5537, 1997, 7817, 2046, 4109, 2107, 2004, 2616, 1010, 3145, 22104, 1010, 15672, 1010, 9255, 1998, 2060, 3787, 2170, 19204, 2015, 1012]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "\n",
    "# You can get the vocab_file from https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=\"bert-base-uncased-vocab.txt\")\n",
    "\n",
    "tokens_test = tokenizer.tokenize(\"Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens.\")\n",
    "print(tokens_test)\n",
    "\n",
    "# The tokens can be converted to IDs: indices assigned to the tokens in the vocab_file\n",
    "\n",
    "ids_test = tokenizer.convert_tokens_to_ids(tokens_test)\n",
    "print(ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the tokenizer to convert the textual examples to features that can be read as input into the BERT model. This part of the code is quite lengthy for the following reasons, among others:\n",
    "* to keep the correspondence between the wordpiece-tokenized words and the whitespace-tokenized words (to be able to put the split words together later on)\n",
    "* to split the context paragraphs into multiple parts in they exceed the `max_query_length`\n",
    "* if the answer ends up being split, figure out which part contains most of the answer\n",
    "* and other book-keeping tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 start_position=None,\n",
    "                 end_position=None,\n",
    "                 is_impossible=None):\n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "        \n",
    "        \n",
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "    \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
    "\n",
    "    # The SQuAD annotations are character based. We first project them to\n",
    "    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
    "    # often find a \"better match\". For example:\n",
    "    #\n",
    "    #   Question: What year was John Smith born?\n",
    "    #   Context: The leader was John Smith (1895-1943).\n",
    "    #   Answer: 1895\n",
    "    #\n",
    "    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
    "    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
    "    # the exact answer, 1895.\n",
    "    #\n",
    "    # However, this is not always possible. Consider the following:\n",
    "    #\n",
    "    #   Question: What country is the top exporter of electornics?\n",
    "    #   Context: The Japanese electronics industry is the lagest in the world.\n",
    "    #   Answer: Japan\n",
    "    #\n",
    "    # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
    "    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
    "    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
    "    # in SQuAD, but does happen.\n",
    "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "    for new_start in range(input_start, input_end + 1):\n",
    "        for new_end in range(input_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "            if text_span == tok_answer_text:\n",
    "                return (new_start, new_end)\n",
    "\n",
    "    return (input_start, input_end)\n",
    "\n",
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    features = []\n",
    "    for (example_index, example) in enumerate(examples):\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = []\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training and example.is_impossible:\n",
    "            tok_start_position = -1\n",
    "            tok_end_position = -1\n",
    "        if is_training and not example.is_impossible:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "            if example.end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "                example.orig_answer_text)\n",
    "\n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = []\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "            segment_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in query_tokens:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training and not example.is_impossible:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                out_of_span = False\n",
    "                if not (tok_start_position >= doc_start and\n",
    "                        tok_end_position <= doc_end):\n",
    "                    out_of_span = True\n",
    "                if out_of_span:\n",
    "                    start_position = 0\n",
    "                    end_position = 0\n",
    "                else:\n",
    "                    doc_offset = len(query_tokens) + 2\n",
    "                    start_position = tok_start_position - doc_start + doc_offset\n",
    "                    end_position = tok_end_position - doc_start + doc_offset\n",
    "            if is_training and example.is_impossible:\n",
    "                start_position = 0\n",
    "                end_position = 0\n",
    "\n",
    "\n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_index=example_index,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=tokens,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position,\n",
    "                    is_impossible=example.is_impossible))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_id:  1000000020\n",
      "example_index:  20\n",
      "doc_span_index:  0\n",
      "tokens:  ['[CLS]', 'what', 'entity', 'provides', 'help', 'with', 'the', 'management', 'of', 'time', 'for', 'new', 'students', 'at', 'notre', 'dame', '?', '[SEP]', 'all', 'of', 'notre', 'dame', \"'\", 's', 'undergraduate', 'students', 'are', 'a', 'part', 'of', 'one', 'of', 'the', 'five', 'undergraduate', 'colleges', 'at', 'the', 'school', 'or', 'are', 'in', 'the', 'first', 'year', 'of', 'studies', 'program', '.', 'the', 'first', 'year', 'of', 'studies', 'program', 'was', 'established', 'in', '1962', 'to', 'guide', 'incoming', 'freshmen', 'in', 'their', 'first', 'year', 'at', 'the', 'school', 'before', 'they', 'have', 'declared', 'a', 'major', '.', 'each', 'student', 'is', 'given', 'an', 'academic', 'advisor', 'from', 'the', 'program', 'who', 'helps', 'them', 'to', 'choose', 'classes', 'that', 'give', 'them', 'exposure', 'to', 'any', 'major', 'in', 'which', 'they', 'are', 'interested', '.', 'the', 'program', 'also', 'includes', 'a', 'learning', 'resource', 'center', 'which', 'provides', 'time', 'management', ',', 'collaborative', 'learning', ',', 'and', 'subject', 'tutor', '##ing', '.', 'this', 'program', 'has', 'been', 'recognized', 'previously', ',', 'by', 'u', '.', 's', '.', 'news', '&', 'world', 'report', ',', 'as', 'outstanding', '.', '[SEP]']\n",
      "token_to_orig_map:  {18: 0, 19: 1, 20: 2, 21: 3, 22: 3, 23: 3, 24: 4, 25: 5, 26: 6, 27: 7, 28: 8, 29: 9, 30: 10, 31: 11, 32: 12, 33: 13, 34: 14, 35: 15, 36: 16, 37: 17, 38: 18, 39: 19, 40: 20, 41: 21, 42: 22, 43: 23, 44: 24, 45: 25, 46: 26, 47: 27, 48: 27, 49: 28, 50: 29, 51: 30, 52: 31, 53: 32, 54: 33, 55: 34, 56: 35, 57: 36, 58: 37, 59: 38, 60: 39, 61: 40, 62: 41, 63: 42, 64: 43, 65: 44, 66: 45, 67: 46, 68: 47, 69: 48, 70: 49, 71: 50, 72: 51, 73: 52, 74: 53, 75: 54, 76: 54, 77: 55, 78: 56, 79: 57, 80: 58, 81: 59, 82: 60, 83: 61, 84: 62, 85: 63, 86: 64, 87: 65, 88: 66, 89: 67, 90: 68, 91: 69, 92: 70, 93: 71, 94: 72, 95: 73, 96: 74, 97: 75, 98: 76, 99: 77, 100: 78, 101: 79, 102: 80, 103: 81, 104: 82, 105: 82, 106: 83, 107: 84, 108: 85, 109: 86, 110: 87, 111: 88, 112: 89, 113: 90, 114: 91, 115: 92, 116: 93, 117: 94, 118: 94, 119: 95, 120: 96, 121: 96, 122: 97, 123: 98, 124: 99, 125: 99, 126: 99, 127: 100, 128: 101, 129: 102, 130: 103, 131: 104, 132: 105, 133: 105, 134: 106, 135: 107, 136: 107, 137: 107, 138: 107, 139: 108, 140: 109, 141: 110, 142: 111, 143: 111, 144: 112, 145: 113, 146: 113}\n",
      "token_is_max_context:  {18: True, 19: True, 20: True, 21: True, 22: True, 23: True, 24: True, 25: True, 26: True, 27: True, 28: True, 29: True, 30: True, 31: True, 32: True, 33: True, 34: True, 35: True, 36: True, 37: True, 38: True, 39: True, 40: True, 41: True, 42: True, 43: True, 44: True, 45: True, 46: True, 47: True, 48: True, 49: True, 50: True, 51: True, 52: True, 53: True, 54: True, 55: True, 56: True, 57: True, 58: True, 59: True, 60: True, 61: True, 62: True, 63: True, 64: True, 65: True, 66: True, 67: True, 68: True, 69: True, 70: True, 71: True, 72: True, 73: True, 74: True, 75: True, 76: True, 77: True, 78: True, 79: True, 80: True, 81: True, 82: True, 83: True, 84: True, 85: True, 86: True, 87: True, 88: True, 89: True, 90: True, 91: True, 92: True, 93: True, 94: True, 95: True, 96: True, 97: True, 98: True, 99: True, 100: True, 101: True, 102: True, 103: True, 104: True, 105: True, 106: True, 107: True, 108: True, 109: True, 110: True, 111: True, 112: True, 113: True, 114: True, 115: True, 116: True, 117: True, 118: True, 119: True, 120: True, 121: True, 122: True, 123: True, 124: True, 125: True, 126: True, 127: True, 128: True, 129: True, 130: True, 131: True, 132: True, 133: True, 134: True, 135: True, 136: True, 137: True, 138: True, 139: True, 140: True, 141: True, 142: True, 143: True, 144: True, 145: True, 146: True}\n",
      "input_ids:  [101, 2054, 9178, 3640, 2393, 2007, 1996, 2968, 1997, 2051, 2005, 2047, 2493, 2012, 10289, 8214, 1029, 102, 2035, 1997, 10289, 8214, 1005, 1055, 8324, 2493, 2024, 1037, 2112, 1997, 2028, 1997, 1996, 2274, 8324, 6667, 2012, 1996, 2082, 2030, 2024, 1999, 1996, 2034, 2095, 1997, 2913, 2565, 1012, 1996, 2034, 2095, 1997, 2913, 2565, 2001, 2511, 1999, 3705, 2000, 5009, 14932, 26612, 1999, 2037, 2034, 2095, 2012, 1996, 2082, 2077, 2027, 2031, 4161, 1037, 2350, 1012, 2169, 3076, 2003, 2445, 2019, 3834, 8619, 2013, 1996, 2565, 2040, 7126, 2068, 2000, 5454, 4280, 2008, 2507, 2068, 7524, 2000, 2151, 2350, 1999, 2029, 2027, 2024, 4699, 1012, 1996, 2565, 2036, 2950, 1037, 4083, 7692, 2415, 2029, 3640, 2051, 2968, 1010, 12317, 4083, 1010, 1998, 3395, 14924, 2075, 1012, 2023, 2565, 2038, 2042, 3858, 3130, 1010, 2011, 1057, 1012, 1055, 1012, 2739, 1004, 2088, 3189, 1010, 2004, 5151, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "input_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "start_position:  111\n",
      "end_position:  113\n",
      "is_impossible:  False\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(train_examples, tokenizer, max_seq_length=384,\n",
    "                                 doc_stride=128, max_query_length=64, is_training=True)\n",
    "\n",
    "ind = 20\n",
    "\n",
    "# A consequtively assigned ID number that starts with 1000000000\n",
    "print(\"unique_id: \", train_features[ind].unique_id)\n",
    "\n",
    "# Looks to be just = unique_id - 1000000000\n",
    "print(\"example_index: \",train_features[ind].example_index)\n",
    "\n",
    "# Documents longer than max_seq_length are split into chunks indexed by doc_span_index with a stride of doc_stride\n",
    "print(\"doc_span_index: \",train_features[ind].doc_span_index)\n",
    "\n",
    "# Tokens for the Question starting with '[CLS]', followed by '[SEP]' and then by the tokens for the Context\n",
    "print(\"tokens: \",train_features[ind].tokens)\n",
    "\n",
    "# { <token # from features[ind].tokens> : <# of a word in the original Context> } \n",
    "# NOTE: When the original word has been split into multiple tokens, those multiple \n",
    "# <token # from features[ind].tokens> correspond to the same <# of a word in the original Context>\n",
    "print(\"token_to_orig_map: \",train_features[ind].token_to_orig_map)\n",
    "\n",
    "# Does this document span contain \"maximum content\" of the answer\n",
    "print(\"token_is_max_context: \",train_features[ind].token_is_max_context)\n",
    "\n",
    "# IDs that the tokens in features[ind].tokens have in the BERT vocabulary \n",
    "# (Additionally, 0s for padding entries that go up to len = max_seq_length)\n",
    "print(\"input_ids: \",train_features[ind].input_ids)\n",
    "\n",
    "# 1 for each non-trivial token, 0 for padding entries that go up to len = max_seq_length\n",
    "print(\"input_mask: \",train_features[ind].input_mask)\n",
    "\n",
    "# 1 for each token that is part of the Context, 0 otherwise\n",
    "print(\"segment_ids: \",train_features[ind].segment_ids)\n",
    "\n",
    "# Start and end positions of the answer\n",
    "print(\"start_position: \",train_features[ind].start_position)\n",
    "print(\"end_position: \",train_features[ind].end_position)\n",
    "\n",
    "# SQuAD 2.0 includes questions that cannot be answered from the context provided\n",
    "# This is the flag for those \"impossible\" questions\n",
    "print(\"is_impossible: \",train_features[ind].is_impossible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The model\n",
    "\n",
    "The way BERT's creators have set it up for the SQuAD task is the following: the tokenized Question and Context are put together into a single input sequence (the two separated by a special token) and a linear layer is added on top of the Encoder (i.e. BERT) which is then used to predict the start and end of the Answer inside the Context.\n",
    "\n",
    "![](squadbert.jpeg)\n",
    "(Fig. from the original [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "### 2A. The pre-trained BERT model for Question Answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from pytorch_transformers.modeling_bert import BertPreTrainedModel, BertModel\n",
    "\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (`sequence_length`).\n",
    "            Position outside of the sequence are not taken into account for computing the loss.\n",
    "        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (`sequence_length`).\n",
    "            Position outside of the sequence are not taken into account for computing the loss.\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n",
    "        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n",
    "            Span-start scores (before SoftMax).\n",
    "        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n",
    "            Span-end scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        >>> \n",
    "        >>> model = BertForQuestionAnswering(config)\n",
    "        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        >>> start_positions = torch.tensor([1])\n",
    "        >>> end_positions = torch.tensor([3])\n",
    "        >>> outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "        >>> loss, start_scores, end_scores = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnswering, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,\n",
    "                end_positions=None, position_ids=None, head_mask=None):\n",
    "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask, head_mask=head_mask)\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        outputs = (start_logits, end_logits,) + outputs[2:]\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n",
    "    \n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "model.half()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with io.capture_output() as captured:    \n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Train the Question Answering model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Num orig examples =  87599\n",
      "  Num split examples =  88641\n",
      "  Batch size =  32\n",
      "  Num steps =  2771\n",
      "Time it took to complete the epoch:  3215.1631803512573\n",
      "Loss after epoch  0 :  0.07489013671875\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from apex import amp\n",
    "from apex.optimizers import FP16_Optimizer, FusedAdam\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "num_train_epochs = 1\n",
    "train_batch_size = 32\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n",
    "all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                   all_start_positions, all_end_positions)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "# hack to remove pooler, which is not used\n",
    "# thus it produce None grad that break apex\n",
    "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                            ]\n",
    "num_train_optimization_steps = len(train_dataloader) *  num_train_epochs\n",
    "\n",
    "optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                                  lr=3e-5,\n",
    "                                  bias_correction=False,\n",
    "                                  max_grad_norm=1.0)\n",
    "optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "print(\"  Num orig examples = \", len(train_examples))\n",
    "print(\"  Num split examples = \", len(train_features))\n",
    "print(\"  Batch size = \", train_batch_size)\n",
    "print(\"  Num steps = \", num_train_optimization_steps)\n",
    "\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch) # multi-gpu does scattering it-self\n",
    "        input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
    "        outputs = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
    "        loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "     \n",
    "    if epoch==0:\n",
    "        print(\"Time it took to complete the epoch: \", (time.time()-start_time))\n",
    "    print(\"Loss after epoch \", epoch, \": \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2C. Save the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output/vocab.txt',\n",
       " 'output/special_tokens_map.json',\n",
       " 'output/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"output/\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D. Test the model on the SQuAD dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running predictions *****\n",
      "  Num orig examples =  10570\n",
      "  Num split examples =  10833\n",
      "  Batch size =  8\n",
      "Start evaluating\n",
      "Processing example: 0\n",
      "Processing example: 1000\n",
      "Processing example: 2000\n",
      "Processing example: 3000\n",
      "Processing example: 4000\n",
      "Processing example: 5000\n",
      "Processing example: 6000\n",
      "Processing example: 7000\n",
      "Processing example: 8000\n",
      "Processing example: 9000\n",
      "Processing example: 10000\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "\n",
    "from run_squad_dataset_utils import read_squad_examples, convert_examples_to_features, RawResult, write_predictions\n",
    "\n",
    "dev_file = \"/root/BERT/SQuAD1/dev-v1.1.json\"\n",
    "predict_batch_size = 8\n",
    "\n",
    "eval_examples = read_squad_examples(input_file=dev_file, is_training=False, version_2_with_negative=False)\n",
    "eval_features = convert_examples_to_features(\n",
    "            examples=eval_examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=384,\n",
    "            doc_stride=128,\n",
    "            max_query_length=64,\n",
    "            is_training=False)\n",
    "\n",
    "print(\"***** Running predictions *****\")\n",
    "print(\"  Num orig examples = \", len(eval_examples))\n",
    "print(\"  Num split examples = \", len(eval_features))\n",
    "print(\"  Batch size = \", predict_batch_size)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=predict_batch_size)\n",
    "\n",
    "model.eval()\n",
    "all_results = []\n",
    "print(\"Start evaluating\")\n",
    "for input_ids, input_mask, segment_ids, example_indices in eval_dataloader:\n",
    "    if len(all_results) % 1000 == 0:\n",
    "        print(\"Processing example: %d\" % (len(all_results)))\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
    "    for i, example_index in enumerate(example_indices):\n",
    "        start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
    "        end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
    "        eval_feature = eval_features[example_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "        all_results.append(RawResult(unique_id=unique_id,\n",
    "                                             start_logits=start_logits,\n",
    "                                             end_logits=end_logits))\n",
    "        \n",
    "output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n",
    "output_null_log_odds_file = os.path.join(output_dir, \"null_odds.json\")\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    write_predictions(eval_examples, eval_features, all_results, 20,\n",
    "                      30, True, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file, True,\n",
    "                      False, 0.0)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"exact_match\": 77.48344370860927, \"f1\": 85.67522993087587}\r\n"
     ]
    }
   ],
   "source": [
    "! python SQuAD1/evaluate-v1.1.py SQuAD1/dev-v1.1.json /root/BERT/output/predictions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning BERT for the Question-Answering task for a **single training epoch** already puts us in the top 40 entries on the SQuAD 1.1 Leaderboard.\n",
    "\n",
    "# 3. The Demo\n",
    "\n",
    "### 3A. Load a saved trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "from pytorch_transformers.modeling_bert import BertForQuestionAnswering\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "\n",
    "output_dir = \"old_output/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForQuestionAnswering.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "with io.capture_output() as captured:    \n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Run inference on arbitrary Question + Context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_seq_length = 384\n",
    "\n",
    "context = \"Tennis is a racket sport that can be played individually against a single opponent (singles) or between two teams of two players each (doubles). Each player uses a tennis racket that is strung with cord to strike a hollow rubber ball covered with felt over or around a net and into the opponent's court. The object of the game is to maneuver the ball in such a way that the opponent is not able to play a valid return. The player who is unable to return the ball will not gain a point, while the opposite player will.\"\n",
    "#input(\"Insert a paragraph: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the goal of a tennis match?\"\n",
    "#input(\"Type in a question that can be answered by a span from a paragraph: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to maneuver the ball in such a way that the opponent is not able to play a valid return \n"
     ]
    }
   ],
   "source": [
    "# TOKENIZE QUESTION AND CONTEXT\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=\"bert-base-uncased-vocab.txt\")\n",
    "query_tokens = tokenizer.tokenize(question)\n",
    "segment_tokens = tokenizer.tokenize(context)\n",
    "\n",
    "tokens = []\n",
    "segment_ids = []\n",
    "tokens.append(\"[CLS]\")\n",
    "segment_ids.append(0)\n",
    "for token in query_tokens:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(0)\n",
    "\n",
    "for token in segment_tokens:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(1)\n",
    "\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(1)\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "# tokens are attended to.\n",
    "input_mask = [1] * len(input_ids)\n",
    "\n",
    "# Zero-pad up to the sequence length.\n",
    "while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "input_mask = torch.tensor(input_mask).unsqueeze(0)\n",
    "segment_ids = torch.tensor(segment_ids).unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "input_mask = input_mask.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "\n",
    "def _get_best_indexes(logits, n_best_size):\n",
    "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_logits, end_logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    start_logits = start_logits.detach().cpu().numpy()\n",
    "    end_logits = end_logits.detach().cpu().numpy()\n",
    "    \n",
    "start = np.argmax(start_logits)\n",
    "end = np.argmax(end_logits)\n",
    "\n",
    "answer = \"\"\n",
    "\n",
    "for i in range(start, end+1):\n",
    "    answer = answer + tokens[i] + \" \"\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
